Big O Notation is a way to scale how well a computer algorithm scales as the amount of data involved increases.Big O is used to describe the efficiency of algorithms.
Big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows.
The letter O is used because the growth rate of a function is also referred to as the order of the function. A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function.
Time Complexity Examples
No matter how big the constant is and how slow the linear increase is, linear will at some point surpass
constant.